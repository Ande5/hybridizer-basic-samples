{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1>Image Processing with Hybridizer</h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image processing is most often an embarassingly parallel problem. It naturally fits on the GPU. \n",
    "\n",
    "In this lab, we will study optimization techniques through the implementation and analysis of the median filter, a robust denoising filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "To get the most out of this lab, you should already be able to:\n",
    "- Write, compile, and run C# programs that both call CPU functions and **launch** GPU **kernels**.\n",
    "- Control parallel **thread hierarchy** using **execution configuration**.\n",
    "- Have some notions on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objectives\n",
    "\n",
    "By the time you complete this lab, you will be able to:\n",
    "- Accelerate image processing algorithms with Hybridizer and GPUs\n",
    "- Explore three different work distribution patterns for image processing\n",
    "- Allocate data into registers\n",
    "- Some profiling elements for pipeline usage and cache usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Median Filter\n",
    "\n",
    "The median filter is a non-linear image filter. It is a robust filter used to remove noise in images. For a given window size, the median of values within that window is used to represent the output. Depending on the size of the window, the results will vary, and a size of 1x1 outputs the same image. Illustration of the filter:\n",
    "\n",
    "<img title=\"median-filter.png\" src=\"./images/median-filter.png\"/>\n",
    "\n",
    "Unlike gaussian or related filters, calculating the median requires a sort, which adds to the complexity for an efficient implementation. It is a very data intensive filter with no arithmetic operation: given window data, the median is extracted, by sorting the values in the window. From an output pixel to an adjacent, most of data is shared and we will see how to make use of this overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Working Set\n",
    "\n",
    "In this lab, we will be processing an reference image (on the left), onto which noise has been artificially added : white pixels have been randomly added on the input image (on the right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:table;margin:0 auto\"><div style=\"display:block;float:left\"><img title=\"lena_highres_greyscale.bmp\" src=\"./images/lena_highres_greyscale.bmp\" width=\"384\"/></div><div style=\"display:block;float:left;margin-left:32px\"><img title=\"lena_highres_greyscale_noise.bmp\" src=\"./images/lena_highres_greyscale_noise.bmp\" width=\"384\"/></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## First Naive Implementation\n",
    "\n",
    "We start the implementation of the filter with a first naive approach as follows:\n",
    "\n",
    "```csharp\n",
    "\n",
    "public static void NaiveCsharp(ushort[] output, ushort[] input, int width, int height)\n",
    "{\n",
    "    int windowCount = 2 * window + 1;\n",
    "    var buffer = new ushort[windowCount * windowCount];\n",
    "    for (int j = window; j < height - window; ++j)\n",
    "    {\n",
    "        for (int i = window; i < width - window; ++i)\n",
    "        {\n",
    "            for (int k = -window; k <= window; ++k)\n",
    "            {\n",
    "                for (int p = -window; p <= window; ++p)\n",
    "                {\n",
    "                    int bufferIndex = (k + window) * windowCount + p + window;\n",
    "                    int pixelIndex = (j + k) * width + (i + p);\n",
    "                    buffer[bufferIndex] = input[pixelIndex];\n",
    "                }\n",
    "            }\n",
    "\n",
    "            Array.Sort(buffer, 0, windowCount * windowCount);\n",
    "            output[j * width + i] = buffer[(windowCount * windowCount) / 2];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "        \n",
    "```\n",
    "\n",
    "This approach has no inherent parallelism, yet each loop iteration is independent. We will focus on the core part of the calculation, and leave borders management outside of the scope of this lab.\n",
    "\n",
    "The [`01-naive-csharp.cs`](../../edit/04_MedianFilter/01-naive/01-naive-csharp.cs) contains a program that is already working. It will load the input noisy image, process it and save the image in an output directory.\n",
    "\n",
    "Use the below code cell to execute the program and display the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "if platform.system() == \"Windows\" : # create directory on Windows\n",
    "    !mkdir output-01-naive \n",
    "if platform.system() == \"Linux\" : # create directory on Linux\n",
    "    !mkdir -p ./output-01-naive \n",
    "\n",
    "!hybridizer-cuda 01-naive-csharp.cs graybitmap.cs -o Target/naive-csharp/01-naive-csharp.exe -run\n",
    "\n",
    "# convert bmp to png to have interactive display\n",
    "from PIL import Image\n",
    "img = Image.open('./output-01-naive/denoised.bmp')\n",
    "img.save('./output-01-naive/denoised.png', 'png')\n",
    "from IPython.display import Image\n",
    "Image(filename=\"./output-01-naive/denoised.png\", width=384, height=384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Distributing on CPU Threads\n",
    "\n",
    "To accelerate calculations on CPU, we can distribute the work on threads. For this, make use [`Parallel.For`](https://msdn.microsoft.com/en-us/library/dd783539.aspx) construct on the lines to process lines in parallel. *note that each thread may require a separate buffer*.\n",
    "\n",
    "Modify `02-parfor-csharp.cs` to make use of CPU parallelism.\n",
    "\n",
    "See the `02-parfor-csharp.cs` file in the `Solutions` directory if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "if platform.system() == \"Windows\" : # create directory on Windows\n",
    "    !mkdir output-02-parfor\n",
    "if platform.system() == \"Linux\" : # create directory on Linux\n",
    "    !mkdir -p ./output-02-parfor\n",
    "\n",
    "!hybridizer-cuda 02-parfor-csharp.cs graybitmap.cs -o Target/parallel-for/02-parfor-csharp.exe -run\n",
    "\n",
    "# convert bmp to png to have interactive display\n",
    "from PIL import Image\n",
    "img = Image.open('./output-02-parfor/denoised.bmp')\n",
    "img.save('./output-02-parfor/denoised.png', 'png')\n",
    "from IPython.display import Image\n",
    "Image(filename=\"./output-02-parfor/denoised.png\", width=384, height=384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Running on the GPU\n",
    "\n",
    "In order to run the filter on the GPU, three operations are needed: \n",
    " - Mark the method with an `EntryPoint` attribute to indicate it should run on GPU\n",
    " - Launch the kernel using the `HybRunner`, generated with static method `HybRunner.Cuda()`\n",
    " - Array.Sort is not a builtin mapped to an existing code, the sort will be changed (*this is out of the scope of this lab and will be already done in the input file*)\n",
    "\n",
    "Creating an instance of HybRunner and wrapping an object is done as follow:\n",
    "```csharp\n",
    "HybRunner runner = HybRunner.Cuda();\n",
    "dynamic wrapper = runner.Wrap(new Program());\n",
    "```\n",
    "Note that the result of the `Wrap` method is a dynamic type generated on the fly by the runner. It exposes the same methods as the wrapped type, with the same signature. Hence, launching the kernel is simply done by calling the method using the `wrapper` instance instead of the base instance (or no instance for static methods).\n",
    "\n",
    "\n",
    "We will start from the `Parallel.For` version of the code. This expression of parallelism is interpreted by Hybridizer and transformed into a grid stride loop on threads and blocks. Hence adding the `EntryPoint` attribute should suffice.\n",
    "\n",
    "\n",
    "Modify `03-naive-gpu.cs` to make sure the method runs on GPU.\n",
    "\n",
    "See the `03-naive-gpu.cs` file in the `Solutions` directory if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "if platform.system() == \"Windows\" : # create directory on Windows\n",
    "    !mkdir output-03-naive-gpu\n",
    "if platform.system() == \"Linux\" : # create directory on Linux\n",
    "    !mkdir -p ./output-03-naive-gpu\n",
    "\n",
    "!hybridizer-cuda 03-naive-gpu.cs graybitmap.cs -intrinsics bitonicsort.cuh=./ -o Target/naive-gpu/03-naive-gpu.exe -run\n",
    "\n",
    "# convert bmp to png to have interactive display\n",
    "from PIL import Image\n",
    "img = Image.open('./output-03-naive-gpu/denoised.bmp')\n",
    "img.save('./output-03-naive-gpu/denoised.png', 'png')\n",
    "from IPython.display import Image\n",
    "Image(filename=\"./output-03-naive-gpu/denoised.png\", width=384, height=384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Memory Allocation\n",
    "\n",
    "The obtained performance appears to be very low. Let's investigate. \n",
    "\n",
    "In the `Parallel.For`, an array is allocated for each line in the image. A `malloc` on the GPU, done for each thread, is very expensive (given execution configuration - we will see that later on - there are thousands of calls). \n",
    "\n",
    "To reduce this, we don't allocate memory dynamically on the heap, but rather on the stack (local memory - or registers in best cases), which is allocated once at kernel startup. To this aim, we expose a class for which constructor will be mapped by a C array declaration:\n",
    "\n",
    "```csharp\n",
    "var buffer = new StackArray<byte>(size) ;\n",
    "```\n",
    "\n",
    "Will get translated into:\n",
    "\n",
    "```c++\n",
    "unsigned char buffer[size] ;\n",
    "```\n",
    "\n",
    "For this declaration to be valid, size shall be a [compile-time constant](http://en.cppreference.com/w/cpp/language/array). There are three ways for obtaining compile-time constants:\n",
    " - Litteral constants, e.g. `buffer = new StackArray<byte>(42)`\n",
    " - Constants defined using the HybridConstant attribute on static data, or IntrinsicConstant attribute on a property or method\n",
    " - Class constants, assuming compiler will replace those during MSIL generation\n",
    " \n",
    "Here is an example\n",
    "```csharp\n",
    "class Filter\n",
    "{\n",
    "    const int window = 5 ;\n",
    "    const int windowCount = 2 * window + 1 ;\n",
    " \n",
    "    [EntryPoint]\n",
    "    public void F()\n",
    "    {\n",
    "        var buffer = new StackArray<byte>(windowCount * windowCount) ;\n",
    "     \n",
    "        ushort[] contents = buffer.data ;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "*This leads to a limitation in variety of filters as the size needs to be provided at compile time. We will discuss this point later on.*\n",
    "\n",
    "Modify `04-stack-gpu.cs` to allocate data on the stack instead of the heap.\n",
    "\n",
    "See the `04-stack-gpu.cs` file in the `Solutions` directory if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "if platform.system() == \"Windows\" : # create directory on Windows\n",
    "    !mkdir output-04-stack-gpu\n",
    "if platform.system() == \"Linux\" : # create directory on Linux\n",
    "    !mkdir -p ./output-04-stack-gpu\n",
    "\n",
    "!hybridizer-cuda 04-stack-gpu.cs graybitmap.cs -intrinsics bitonicsort.cuh=./ -o Target/stack-gpu/04-stack-gpu.exe -run\n",
    "\n",
    "# convert bmp to png to have interactive display\n",
    "from PIL import Image\n",
    "img = Image.open('./output-04-stack-gpu/denoised.bmp')\n",
    "img.save('./output-04-stack-gpu/denoised.png', 'png')\n",
    "from IPython.display import Image\n",
    "Image(filename=\"./output-04-stack-gpu/denoised.png\", width=384, height=384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feeding the Beast\n",
    "\n",
    "A modern GPU is made of thousands of CUDA-cores, for which operations need to be stacked to hide latency. In our image processing example, the distribution of work is done on lines, that is a couple of thousand for the whole image. Then, each CUDA-thread will process a complete line, as illustrated in the following image:\n",
    "\n",
    "<img title=\"SlicingWork\" src=\"./images/work-stripes.png\"/>\n",
    "\n",
    "Using CUDA API, we may query the number of CUDA cores multiprocessors, and the number of CUDA cores. HybRunner has a default execution configuration that is suited to most use-cases. \n",
    "\n",
    "Run the following [`code`](../../edit/04_MedianFilter/05-dice-gpu/01-query-config.cs) to query information on the GPU and execution configuration - see also [`cudaGetDeviceProperties`](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g1bf9d625a931d657e08db2b4391170f0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hybridizer-cuda 05-query-config.cs -o Target/query-config/05-query-config.exe -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributing 1960 lines by blocks of 128 threads results in 16 busy blocks, which uses a fraction of most GPUs and does not hide latency.\n",
    "\n",
    "Extracting more parallism can be achieved by distributing the work dicing the image in little squares instead of stripes as illustrated here:\n",
    "\n",
    "<img title=\"SlicingWork\" src=\"./images/work-dice.png\"/>\n",
    "\n",
    "The amount of work can be distributed up to 4 Million entries which is sufficiently above the number of GPUs working units.\n",
    "\n",
    "In order to enable this with little effort, `Parallel2D` class exposes a static method `For`, very similar to `System.Threading.Parallel.For` that runs an action over a 2D domain:\n",
    "\n",
    "```csharp\n",
    "[EntryPoint]\n",
    "public static void Parallel2DStack(...)\n",
    "{\n",
    "    Parallel2D.For(fromI,toI, fromJ,toJ, (i,j) => \n",
    "    {\n",
    "        ... // action to be executed for (i,j) domain\n",
    "    });\n",
    "}\n",
    "```\n",
    "\n",
    "Effectively dicing the processing, the execution configuration needs to be modified with `SetDistrib`. Both `X` and `Y` dimensions are used. \n",
    "\n",
    "```csharp\n",
    "dim3 grid = new dim3(<nb blocks X>, <nb blocks Y>, <nb blocks Z - ignored>) ;\n",
    "dim3 block = new dim3(<nb threads X>, <nb threads Y>, <nb threads Z>) ;\n",
    "\n",
    "wrapper.SetDistrib (grid,block) ;\n",
    "```\n",
    "\n",
    "Modify `05-dice-gpu.cs` to use a Parallel2D pattern. You may want to try different values for dimension `X` and `Y`.\n",
    "\n",
    "See the `05-dice-gpu.cs` file in the `Solutions` directory if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "if platform.system() == \"Windows\" : # create directory on Windows\n",
    "    !mkdir output-05-dice-gpu\n",
    "if platform.system() == \"Linux\" : # create directory on Linux\n",
    "    !mkdir -p ./output-05-dice-gpu\n",
    "\n",
    "!hybridizer-cuda 05-dice-gpu.cs graybitmap.cs -intrinsics bitonicsort.cuh=./ -o Target/dice-gpu/05-dice-gpu.exe -run\n",
    "\n",
    "# convert bmp to png to have interactive display\n",
    "from PIL import Image\n",
    "img = Image.open('./output-05-dice-gpu/denoised.bmp')\n",
    "img.save('./output-05-dice-gpu/denoised.png', 'png')\n",
    "from IPython.display import Image\n",
    "Image(filename=\"./output-05-dice-gpu/denoised.png\", width=384, height=384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Profiling application\n",
    "\n",
    "Profiling the application may be done with nvprof. The following execution box provides the command line. We are querying the following metrics: \n",
    " - local_{load,store}_transactions : to get the memory transactions on local memory\n",
    " - {gld_gst}_transactions : to get the memory transactions to global memory - cache misses\n",
    " - l2_{read,write}_transactions : to get the memory transactions on the L2 cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd Target/dice-gpu/hybrid ; nvprof --profile-child-processes --metrics local_load_transactions,local_store_transactions,gld_transactions,gst_transactions,l2_read_transactions,l2_write_transactions ./run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Static Sort\n",
    "\n",
    "We can see that the number of local transactions are many: the local stack buffer has not been placed in registers. Indeed, when sorting the buffer, the size of the buffer is not known statically. We may change this with an alternate sort implementation: a static sort.\n",
    "\n",
    "We can use a static sort template class with an intrinsic type:\n",
    "\n",
    "```csharp\n",
    "public class StaticSort\n",
    "{\n",
    "    [IntrinsicFunction(\"::hybridizer::StaticSort<49>::sort<uint16_t>\")]\n",
    "    public static void Sort(ushort[] data)\n",
    "    {\n",
    "        Array.Sort(data);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "In the above code, when calling method Sort running C# with the dot net virtual machine, the `Array.Sort` method gets called. When Hybridizer processes this method, it replaces the call to `StaticSort.Sort` by a call to the intrinsic function with same parameters, here: `::hybridizer::StaticSort<49>::sort<uint16_t>`, which is a template method of a template type.\n",
    "\n",
    "Modify `06-regsort-gpu.cs` to make use of static sort instead of the bitonic sort.\n",
    "\n",
    "See the `06-regsort-gpu.cs` file in the `Solutions` directory if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "if platform.system() == \"Windows\" : # create directory on Windows\n",
    "    !mkdir output-06-regsort-gpu\n",
    "if platform.system() == \"Linux\" : # create directory on Linux\n",
    "    !mkdir -p ./output-06-regsort-gpu\n",
    "\n",
    "!hybridizer-cuda 06-regsort-gpu.cs graybitmap.cs -o Target/regsort-gpu/06-regsort-gpu.exe -run\n",
    "\n",
    "# convert bmp to png to have interactive display\n",
    "from PIL import Image\n",
    "img = Image.open('./output-06-regsort-gpu/denoised.bmp')\n",
    "img.save('./output-06-regsort-gpu/denoised.png', 'png')\n",
    "from IPython.display import Image\n",
    "Image(filename=\"./output-06-regsort-gpu/denoised.png\", width=384, height=384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd Target/regsort-gpu/hybrid ; nvprof --profile-child-processes --metrics local_load_transactions,local_store_transactions,gld_transactions,gst_transactions,l2_read_transactions,l2_write_transactions ./run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local load and store transactions should have been reduced to zero.\n",
    "\n",
    "The display of kernel time actually includes some memory transfer and synchronization. The real execution time is returned by the profiler when run in summary mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd Target/regsort-gpu/hybrid ; nvprof -s --profile-child-processes  ./run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cache Coherence\n",
    "\n",
    "GPUs are equiped with L1 and L2 caches, which are used automatically. There are other cache types such as the texture cache or the constant cache. All of these will help improve data locality.\n",
    "\n",
    "In the context of this application, the traversal of data can be predicted. Instead of dicing the image, an have all threads load around fifty values (in our example), we can arrange calculations to reuse the previously loaded data, and recude the cache pressure from 49 down to 7.\n",
    "\n",
    "For this, we take a bit more control on the parallel loop using explicit work distribution:\n",
    "\n",
    "```csharp\n",
    "for (int blockJ = blockIdx.x * chunk; blockJ < height; blockJ += gridDim.x * chunk)\n",
    "{\n",
    "    for (int blockI = threadIdx.x; blockI < width; blockI += blockDim.x)\n",
    "    {\n",
    "        <... process the shaft ...>\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The block j index distributes column parts accross blocks, and block i distributes colmumns accross threads.\n",
    "\n",
    "Each thread is in charge of a column part of the image, iterating on the processing line with 7 loads into our register table at each iteration.\n",
    "\n",
    "Finish implementation of `07-cache-aware-gpu.cs` defined an intrinsic type for the rolling buffer.\n",
    "\n",
    "See the `07-cache-aware-gpu.cs` file in the `Solutions` directory if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "if platform.system() == \"Windows\" : # create directory on Windows\n",
    "    !mkdir output-07-cache-aware-gpu\n",
    "if platform.system() == \"Linux\" : # create directory on Linux\n",
    "    !mkdir -p ./output-07-cache-aware-gpu\n",
    "\n",
    "!hybridizer-cuda 07-cache-aware-gpu.cs -intrinsics intrinsics.cuh=./ -backendOptions \"-I\"\"..\\..\\..\"\"\" graybitmap.cs -o Target/cache-aware-gpu/07-cache-aware-gpu.exe -run\n",
    "\n",
    "# convert bmp to png to have interactive display\n",
    "from PIL import Image\n",
    "img = Image.open('./output-07-cache-aware-gpu/denoised.bmp')\n",
    "img.save('./output-07-cache-aware-gpu/denoised.png', 'png')\n",
    "from IPython.display import Image\n",
    "Image(filename=\"./output-07-cache-aware-gpu/denoised.png\", width=384, height=384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd Target/cache-aware-gpu/hybrid ; nvprof --profile-child-processes --metrics local_load_transactions,local_store_transactions,gld_transactions,gst_transactions,l2_read_transactions,l2_write_transactions ./run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of `gld_transactions` and `l2_read_transactions` should significantly reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd Target/cache-aware-gpu/hybrid ; nvprof -s --profile-child-processes  ./run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to query the utilization of the different pipelines. In this last version, we should have a high utilization on the `single_precision_fu_utilization`, and low utilization for the other units. We are compute bound !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd Target/cache-aware-gpu/hybrid ; nvprof --profile-child-processes --metrics ldst_fu_utilization,single_precision_fu_utilization ./run.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
